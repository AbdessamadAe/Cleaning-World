# Part 3 & 4: Parser and Static Semantics Analyzer - Deliverables

## Part 3: Parser

### 1. Parser Description & Choice

**Parser Type:** LALR(1) Parser Generator  
**Tool:** PLY (Python Lex-Yacc) version 3.11  
**Language:** Python 3.11

#### Why LALR(1) with PLY?

1. **Existing Foundation:** The parser was already implemented using PLY in the repository, demonstrating that the grammar is suitable for LALR(1) parsing.
2. **Grammar Strength:** The grammar is free of left recursion and unambiguous, making it ideal for LALR(1).
3. **Production Quality:** LALR(1) provides a good balance between simplicity and expressiveness for this language.
4. **Tool Maturity:** PLY is a well-established, reliable parser generator for Python.

#### How It Works

1. **Lexer → Tokens:** The lexer (`Part1&2/lexer/lexer.py`) reads the source file character by character and produces a token stream using PLY's lex engine.
2. **Parser Rules:** Grammar rules defined in `Part3&4/parser/parser.py` as Python functions (`p_*`) are converted to an LALR(1) parse table.
3. **Parsing:** The parser consumes tokens and builds a Concrete Syntax Tree (CST) by reducing tokens according to the grammar rules.
4. **CST Output:** Each CST node is a `CSTNode` object with fields:
   - `type`: The node's category (e.g., `'program'`, `'if_stmt'`, `'identifier'`)
   - `children`: List of child nodes
   - `value`: Semantic value (identifiers, literals, operators)
   - `lineno`: Source line number for error reporting
5. **File Output:** The parser writes each CST to a readable text file in `Part3&4/CSTs/` directory.

### 2. Parser Files & Setup

#### Required Files
- `Part3&4/parser/parser.py` — Grammar rules and parsing logic
- `Part3&4/parser/parsetab.py` — Generated LALR(1) parse table (auto-generated by PLY)
- `Part1&2/lexer/lexer.py` — Tokenizer
- `Part1&2/lexer/tokens.py` — Token definitions
- `Part3&4/CSTs/` — Output directory for CST files (created automatically by parser)

#### Dependencies
```bash
pip install ply
```

#### To Run Parser
```bash
cd Part3&4
python app.py programs/program1.cl
```

Output: CST written to `CSTs/program1_cst.txt` and printed to stdout.

#### Direct Python Usage
```python
from parser.parser import parse

# Parse a file and get CST
cst = parse(filename='programs/program1.cl')
print(cst)  # Prints readable CST
```

### 3. Test Program Coverage

Four test programs have been created to validate parser coverage:

| Program | File | Focus | Lines |
|---------|------|-------|-------|
| 1 (Original) | `program1.cl` | Basic agent, loops, conditionals, arithmetic | 22 |
| 2 (Functions) | `program2.cl` | Function decl, parameters, returns, calls | 18 |
| 3 (Navigation) | `program3.cl` | TURN LEFT/RIGHT, BACKTRACK statements | 14 |
| 4 (Complex Logic) | `program4.cl` | OBSTACLE, relops (LT, GT, EQ, NEQ), AND/OR logic | 41 |

**Parse Status:** ✓ All 4 programs parse successfully (100%)

### 4. Production Coverage Table

See `PRODUCTION_MAPPING.md` for a detailed mapping of grammar productions to test lines/programs.

**Coverage Summary:**
- **Productions Tested:** 50 out of ~60 (~83%)
- **Untested Productions:** 10 (mostly edge cases like `NOT` operator, single-param functions, void return)

### 5. CST Examples

#### CST for program1.cl (program1_cst.txt)
```
program [line 1]
  world_def: SmallRoom [line 1]
    world_body [line 2]
      size_decl: (5, 5) [line 2]
      entry_decl: (1, 1, 'N') [line 3]
      exit_decl: (5, 5, 'S') [line 4]
      dirt_decl: (3, 3) [line 5]
  function_list_opt
  agent_def: BasicCleaner [line 8]
    stmt_list [line 9]
      var_decl: dirt_collected [line 9]
        integer_literal: 0 [line 9]
      while_stmt [line 11]
        unvisited_condition [line 11]
        stmt_list [line 12]
          if_stmt [line 12]
            sense_condition [line 12]
              dirt_sense: DIRT [line 12]
            ... (rest of tree)
```

**CSTs for all 4 programs:** Available in `Part3&4/CSTs/` directory:
- `program1_cst.txt`
- `program2_cst.txt`
- `program3_cst.txt`
- `program4_cst.txt`

---

## Part 4: Static Semantics Analyzer

### 1. Semantic Analyzer Overview

The static semantics analyzer performs three key functions:

1. **CST → AST Transformation:** Converts the concrete syntax tree (parser output) into a simpler, abstract syntax tree suitable for code generation.
2. **Scoping & Declaration Checks:** Verifies that all identifiers (variables, functions) are properly declared before use.
3. **Type & Signature Validation:** Checks function call signatures match declarations and return types are handled correctly.

### 2. Implementation Details

#### Symbol Table

**Structure:** Nested scopes (list of dictionaries)
- Global scope: stores functions and global variables
- Function scopes: store parameters and local variables
- Agent scope: stores local variables

**Methods:**
- `declare(name, info)`: Add name to current scope; return False if duplicate
- `lookup(name)`: Search for name in current and outer scopes
- `push()`: Enter new scope (for functions/agent)
- `pop()`: Exit current scope

#### Analysis Phases

1. **Phase 1: Function Registration**
   - Scan all function declarations
   - Register function names, parameter counts, and return types in global symbol table
   - Detect duplicate function declarations

2. **Phase 2: World Transformation**
   - Transform world definition from CST to AST

3. **Phase 3: Function Body Analysis**
   - For each function:
     - Push new scope for function body
     - Declare parameters in function scope
     - Transform function body (statements)
     - Check for undefined identifiers
     - Validate return statements
     - Pop scope

4. **Phase 4: Agent Analysis**
   - Push scope for agent body
   - Transform agent statements
   - Check for undefined identifiers and function calls
   - Pop scope

#### CST → AST Transformation

The analyzer walks the CST and produces simplified AST nodes:

**Example CST → AST:**
```
CST: var_decl: dirt_collected [value=node.value, child=expr]
AST: VarDecl(name, expr)  -- cleaner, flattened

CST: plus_expr: + [children=[left_expr, right_expr]]
AST: BinOp('+', left, right)  -- normalized

CST: if_stmt [children=[cond, then_stmts, else_stmts]]
AST: IfStmt(cond, then_stmts, else_stmts)  -- same structure, transformed children
```

### 3. Semantic Checks Implemented

| Check | Example | Error Message |
|-------|---------|---------------|
| Undefined variable in assignment | `x = 5;` (x not declared) | "Assignment to undeclared identifier: x" |
| Undefined variable in expression | `VAR y = x + 1;` (x not declared) | "Use of undeclared identifier: x" |
| Undefined function call | `result = foo(1);` (foo not declared) | "Call to undeclared function: foo" |
| Duplicate function declaration | Two `FUNC add(...)` | "Duplicate function declaration: add" |
| Duplicate variable declaration | Two `VAR x = ...` in same scope | "Duplicate variable declaration: x" |
| Duplicate parameter name | `FUNC f(X, X) RETURNS INT` | "Duplicate parameter name 'X' in function f" |
| RETURN outside function | `RETURN 0;` in agent body | "RETURN used outside of function" |
| Void function with return value | `FUNC f() RETURNS VOID { RETURN 42; }` | "Function f is void but RETURN has a value" |
| Function arg count mismatch | `add(x)` but `add(X, Y)` defined | "Function add called with 1 args but expects 2" |

### 4. Semantic Analyzer Files

- `Part3&4/semantics_analyzer/semantic.py` — Main analyzer (270 lines)
- `Part3&4/semantics_analyzer/ast_nodes.py` — AST node constructors (80 lines)

#### To Run Analyzer

```python
from parser.parser import parse
from semantics_analyzer.semantic import analyze_cst

cst = parse(filename='programs/program1.cl')
ast, errors = analyze_cst(cst)

if ast:
    print("AST:", ast)
if errors:
    print("Errors:", errors)
```

### 5. AST Examples

#### AST for program1.cl

```
Program
  WorldDef: SmallRoom
    Size: (5, 5)
    Entry: (1, 1, 'N')
    Exit: (5, 5, 'S')
    Dirt: (3, 3)
  Functions
  Agent: BasicCleaner
    VarDecl: dirt_collected
      Int: 0
    While
      Unvisited
      Body
        If
          Sense: DIRT
          Then
            Clean
            Assign: dirt_collected
              BinOp: +
                Var: dirt_collected
                Int: 1
          Else
            Move
    Report
      Var: dirt_collected
```

#### AST for program2.cl (with functions)

```
Program
  WorldDef: FunctionTest
    Size: (10, 10)
    Entry: (1, 1, 'N')
    Exit: (10, 10, 'S')
  Functions
    Function: add
      Params
        Param: X
        Param: Y
      RetType: int
      Body
        Return
          BinOp: +
            Var: X
            Var: Y
  Agent: FunctionAgent
    VarDecl: result
      Int: 0
    ... (rest of agent body)
    Assign: result
      Call: add
        Var: x
        Var: y
```

### 6. Test Results

**All 4 programs analyzed successfully:**

| Program | Semantic Errors | Parse Status | AST Generated |
|---------|-----------------|--------------|---------------|
| program1.cl | 0 | ✓ | ✓ |
| program2.cl | 0 | ✓ | ✓ |
| program3.cl | 0 | ✓ | ✓ |
| program4.cl | 0 | ✓ | ✓ |

**Overall: 0 semantic errors across all programs**

---

## System Integration: Lexer → Parser → Analyzer

The three stages are connected as follows:

```
Source Code (.cl file)
         ↓
    [LEXER] (Part1&2/lexer/lexer.py)
       ↓ (token stream)
    [PARSER] (Part3&4/parser/parser.py)
       ↓ (Concrete Syntax Tree)
    [SEMANTIC ANALYZER] (Part3&4/semantics_analyzer/semantic.py)
       ↓ (Abstract Syntax Tree + Errors)
    Output: AST and semantic error report
```

**Connection Method:** In-memory objects
- Lexer produces tokens that parser consumes
- Parser produces CST that analyzer consumes
- No intermediate files required (optional: write CST to file for visualization)

**Entry Point:** `Part3&4/app.py`
```python
def run_file(path):
    cst = parse(filename=path)
    ast, errors = analyze_cst(cst)
    print(ast, errors)
```

---

## Deliverable Files & Locations

### Part 3 (Parser)

| Item | Location |
|------|----------|
| Parser source | `Part3&4/parser/parser.py` |
| Parse table (auto-gen) | `Part3&4/parser/parsetab.py` |
| CST for program1 | `Part3&4/CSTs/program1_cst.txt` |
| CST for program2 | `Part3&4/CSTs/program2_cst.txt` |
| CST for program3 | `Part3&4/CSTs/program3_cst.txt` |
| CST for program4 | `Part3&4/CSTs/program4_cst.txt` |
| Production mapping | `Part3&4/PRODUCTION_MAPPING.md` |

### Part 4 (Semantic Analyzer)

| Item | Location |
|------|----------|
| Analyzer source | `Part3&4/semantics_analyzer/semantic.py` |
| AST node definitions | `Part3&4/semantics_analyzer/ast_nodes.py` |
| Test programs | `Part3&4/programs/program1.cl` - `program4.cl` |

---

## Known Limitations & Future Enhancements

1. **Type System:** Currently, all variables are assumed to be `int`. Extensible to support multiple types.
2. **Semantic Checks:** No deep type inference or cross-expression type checking.
3. **Operator Validation:** No validation that arithmetic operators work on correct types.
4. **Forward Declarations:** Functions must be declared before use (single-pass analysis).
5. **Void Functions:** Void return type allowed but not deeply validated.

**Recommended Enhancements:**
- Add type tracking to symbol table entries
- Implement cross-expression type checking
- Add warnings for unused variables
- Implement dead code detection

---

## Quick Start

```bash
# Install dependencies
pip install ply

# Parse program1.cl and generate CST
cd Part3&4
python app.py programs/program1.cl

# Expected output:
# CST written to: CSTs/program1_cst.txt
# (CST printed to stdout)
# No semantic errors
```

---

**Document Version:** 1.0  
**Date:** 2025-11-30  
**Parser & Analyzer Status:** Production Ready ✓
